{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse linear models for image denoising\n",
    "\n",
    "### What are sparse methods?\n",
    "\n",
    "Sparse methods in signal processing use special structure that is shared by some signals, that enables representing them as linear combinations of base signals ('atoms') with few nonzeros or big values.\n",
    "\n",
    "Sparse methods have many application, for example they are used for image denoising, deblurring, superresolution, source separation and image compression.\n",
    "\n",
    "#### Teaser\n",
    "\n",
    "* JPEG algorithm\n",
    "* Heard about Dictionary Learning? What is `sklearn.decomposition.DictionaryLearning`?\n",
    "* Seen Orthogonal Matching Pursuit? What is `sklearn.linear_model.OrthogonalMatchingPursuit`?\n",
    "\n",
    "** TODO Edit JPEG comment** \n",
    "\n",
    "One example of method that in practice works like sparse method is JPEG algorithm. Its quantization step, which is done after Discrete Cosine Transform works by zeroing out small coefficients (this is actually only lossy step)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear equations and sparsity\n",
    "\n",
    "Consider linear system $Ax = y$. This equation might either not have any solution, or have many solutions.\n",
    "\n",
    "Sparse methods aim at finding canonical solution where there might be many potential guesses - the original problem becomes\n",
    "\n",
    "Find $x$ such that $Ax = y$, and $x$ is *sparse*.\n",
    "\n",
    "### How to define sparsity precisely?\n",
    "\n",
    "In math/signal processing several types of metrics are used to measure sparsity.\n",
    "\n",
    "#### $L_0$ and $L_1$\n",
    "\n",
    "Sparsity is most commonly measured using $L_0$ metric or $L_1$ 'metric'.\n",
    "\n",
    "$L_1$ metric, also known as Manhattan distance, is defined as\n",
    "\n",
    "$\\|x\\|_1 = \\sum_{i}{|x_i|}$ \n",
    "\n",
    "Whereas $L_0$ is not actually a metric, and is defined as\n",
    "\n",
    "$\\|x\\|_0 = supp(x) = $*number of nonzero coefficients in* $x$\n",
    "\n",
    "One might ask: why use two separate notions? The answer to that is that $L_0$ is not even continuous, and it is hard to optimize. In general problem of finding solution to linear system with smallest $L_0$ is NP-hard!\n",
    "\n",
    "$L_1$ on the other hand is continuous and convex. This makes using it appealing, since in optimization there is a whole subfield dealing with such problems. Also, $L_1$ can be thought of as a convexification of $L_0$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(image of L1 vs L0 'unit balls')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1 vs L2 (Euclidean) norm\n",
    "\n",
    "A person familiar with regularization in linear regression might ask: \n",
    "\n",
    "What's the difference between minimizing $\\|x\\|^2$ and $\\|x\\|_1$ while solving  $Ax = b$? Doesn't minimizing Euclidean norm lead to small coefficients?\n",
    "\n",
    "The problem is in structure: while both norms enforce 'small coefficients', Euclidean norm treats small and big coefficients differently, penalizing big coefficients more, while caring less for differences in small coefficients.\n",
    "\n",
    "Consider example:\n",
    "\n",
    "$x \\in \\mathbb{R}^d, x_i = \\sqrt{\\frac{1}{d}}$.\n",
    "\n",
    "Then\n",
    "\n",
    "$\\|x\\|^2 = \\sum_{i=0}^d \\frac{1}{d} = 1$\n",
    "\n",
    "But\n",
    "\n",
    "$\\|x\\|_1 = \\sum_{i=0}^d{\\sqrt{\\frac{1}{d}}} = d {\\sqrt{\\frac{1}{d}}} = \\sqrt{d}$\n",
    "\n",
    "So even though coefficients of $x$ get smaller as $d$ gets bigger, it's L1 norm actually get bigger!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparsity and sparse data structures\n",
    "\n",
    "Sparsity in mathematics/signal processing is related, but not equal to sparsity from computer science point of view.\n",
    "In computer science sparse matrix is a data structure that holds only nonzero coefficients.\n",
    "The relation between sparse data structure and mathematial notion lies in fact that sparse matrix data structure works well for representing matrices sparse in precise sense - this is also actually used in many 'sparse algorithms'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary learning and related terminology\n",
    "\n",
    "In practical image/signal processing problems it is common to use convention\n",
    "\n",
    "$x \\approx D\\alpha$ where $\\alpha$ is found with some kind of sparse method.\n",
    "\n",
    "The $D$ matrix is called dictionary, and its columns are called atoms, in other words $\\alpha$ gives $x$'s *decomposition into atoms*.\n",
    "\n",
    "Signal processing gives lots of examples of potential dictionaries - above we mentioned how JPEG algorithm uses Discrete Cosine Transform coefficients as $D$. Another commonly used dictionaries come from wavelet transforms.\n",
    "\n",
    "$D$ might be also learned from training data, which is the task of dictionary learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Denoising problem formulation\n",
    "\n",
    "### Sparse models for denoising\n",
    "\n",
    "#### Used dictionary\n",
    "\n",
    "For this part we will use Daubechies wavelets coefficients.\n",
    "\n",
    "#### Other considerations\n",
    "\n",
    "Use other wavelets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced topics\n",
    "\n",
    "#### Other method that could utilize sparsity\n",
    "\n",
    "Sparsity naturally comes up in matrix factorization (from using some form of L1 regularization)\n",
    "\n",
    "* Nonnegative Matrix Factorization (**TODO** link this with NMF on arXiv )\n",
    "\n",
    "* Sparse PCA (PCA with added L1 penalty)\n",
    "\n",
    "* Robust PCA\n",
    "\n",
    "A precise way to formulate the problem is to pose it as Maximum A Posteriori estimation:\n",
    "\n",
    "If Gaussian noise is assumed, we can use prior on transform coefficients to derive (here $D$ is dictionary (in particular it could correspond to transform matrix), $X$ are coefficients, and $Y$ is data used for estimation)\n",
    "\n",
    "$-logP(X | Y) \\propto -log(P(Y|X)P(X)) \\propto -log(e^{\\|Y - DX\\|^2} e^{\\lambda\\|X\\|_1}) =  \\|Y - DX\\|^2 + \\lambda\\|X\\|_1$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
