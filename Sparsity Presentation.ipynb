{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sparse linear models for image denoising\n",
    "\n",
    "### What are sparse methods?\n",
    "\n",
    "Sparse methods in signal processing use special structure that is shared by some signals, that enables representing them as linear combinations of base signals ('atoms') with few nonzeros or big values.\n",
    "\n",
    "Sparse methods have many applications: they are used for image denoising, deblurring, superresolution, source separation and image compression. There is even a style transfer method based on this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Teaser\n",
    "\n",
    "* Heard about Dictionary Learning? What is `sklearn.decomposition.DictionaryLearning`?\n",
    "* Seen Orthogonal Matching Pursuit? What is `sklearn.linear_model.OrthogonalMatchingPursuit`?\n",
    "* JPEG algorithm\n",
    "\n",
    "One example of method that in practice works like sparse method is JPEG algorithm. Its quantization step, which is done after Discrete Cosine Transform works by zeroing out small coefficients (this is actually only lossy step). In fact, this example is equivalent to thresholding method, which we will cover later.\n",
    "\n",
    "** TODO add JPEG algorithm pipeline** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Linear equations and sparsity\n",
    "\n",
    "Consider linear system $Ax = y$. For now we'll assume that this system has solution, but it might have many such solutions.\n",
    "\n",
    "Sparse methods aim at finding canonical solution where there might be many potential guesses - the original problem becomes\n",
    "\n",
    "Find $x$ such that $Ax = y$, and $x$ is *sparse*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How to define sparsity precisely?\n",
    "\n",
    "In math/signal processing several types of metrics are used to measure sparsity.\n",
    "\n",
    "#### $L_0$ and $L_1$\n",
    "\n",
    "Sparsity is most commonly measured using $L_0$ metric or $L_1$ 'metric'.\n",
    "\n",
    "$L_1$ metric, also known as Manhattan distance, is defined as\n",
    "\n",
    "$\\|x\\|_1 = \\sum_{i}{|x_i|}$ \n",
    "\n",
    "Whereas $L_0$ is not actually a metric, and is defined as\n",
    "\n",
    "$\\|x\\|_0 = supp(x) =$ *number of nonzero coefficients in* $x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "One might ask: why use two separate notions? The answer to that is that $L_0$ is not even continuous, and it is hard to optimize. In general problem of finding solution to linear system with smallest $L_0$ is NP-hard!\n",
    "\n",
    "$L_1$ on the other hand is continuous and convex. This makes using it appealing, since in optimization there is a whole subfield dealing with such problems. Also, $L_1$ can be thought of as a convexification of $L_0$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "(image of L1 vs L0 'unit balls')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In fact, finding $x$ such that\n",
    "\n",
    "$Ax = b$ ,  $\\|x\\|_1$ minimal\n",
    "\n",
    "can be solved with linear program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### L1 vs L2 (Euclidean) norm\n",
    "\n",
    "A person familiar with regularization in linear regression might ask: \n",
    "\n",
    "What's the difference between minimizing $\\|x\\|^2$ and $\\|x\\|_1$ while solving  $Ax = b$? Doesn't minimizing Euclidean norm lead to small coefficients?\n",
    "\n",
    "The problem is in structure: while both norms enforce 'small coefficients', Euclidean norm treats small and big coefficients differently, penalizing big coefficients more, while caring less for differences in small coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## L1 vs L2 - example\n",
    "\n",
    "$x \\in \\mathbb{R}^d, x_i = \\sqrt{\\frac{1}{d}}$.\n",
    "\n",
    "Then\n",
    "\n",
    "$\\|x\\|^2 = \\sum_{i=0}^d \\frac{1}{d} = 1$\n",
    "\n",
    "But\n",
    "\n",
    "$\\|x\\|_1 = \\sum_{i=0}^d{\\sqrt{\\frac{1}{d}}} = d {\\sqrt{\\frac{1}{d}}} = \\sqrt{d}$\n",
    "\n",
    "So even though coefficients of $x$ get smaller as $d$ gets bigger, it's L1 norm actually get bigger!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Comment on sparse data structures\n",
    "\n",
    "Sparsity in mathematics/signal processing is related, but not equal to sparsity from computer science point of view.\n",
    "In computer science sparse matrix is a data structure that holds only nonzero coefficients.\n",
    "The relation between sparse data structure and mathematial notion lies in fact that sparse matrix data structure works well for representing matrices sparse in precise sense - this is also actually used in many 'sparse algorithms'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dictionary learning and related terminology\n",
    "\n",
    "In practical image/signal processing problems it is common to use convention\n",
    "\n",
    "$x \\approx D\\alpha$ where $\\alpha$ is found with some kind of sparse method.\n",
    "\n",
    "The $D$ matrix is called dictionary, and its columns are called atoms, in other words $\\alpha$ gives $x$'s *decomposition into atoms*.\n",
    "\n",
    "Signal processing gives lots of candidates for dictionaries - above we mentioned how JPEG algorithm uses Discrete Cosine Transform coefficients as $D$. Another commonly used dictionaries come from wavelet transforms.\n",
    "\n",
    "$D$ might be also learned from training data, which is the task of dictionary learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Denoising problem formulation\n",
    "\n",
    "In denoising most commonly we pose problem as\n",
    "\n",
    "$y = x_{noisy} = x + \\epsilon$ where $\\epsilon$ has iid. normally distributed coefficients with zero mean and std. dev $\\sigma$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sparse models for denoising\n",
    "\n",
    "Using our terminology we pose the problem as\n",
    "\n",
    "Minimize $\\|x - D\\alpha\\|^2$ with $\\|\\alpha\\|_0 \\leq k$\n",
    "\n",
    "#### Optimization algorithm\n",
    "\n",
    "One of the simplest algorithms for solving such problems is thresholding algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def thresholding(D, y, t):\n",
    "    coeffs = D.T @ y\n",
    "    mask = np.abs(coeffs) > t\n",
    "    return coeffs * mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note that this algorithm naturally enforces sparsity - as `t` increases, it can only zero out more coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Used dictionary\n",
    "\n",
    "For this part we will use Daubechies wavelets coefficients.\n",
    "\n",
    "#### How can it be extended\n",
    "\n",
    "* Use different wavelet transform\n",
    "\n",
    "* Use patch-based processing, and then average the patches (`sklearn.feature_extraction.image.extract_patches_2d`)\n",
    " \n",
    "* Use different algorithm (these methods are called *pursuit algorithms*), for example Orthogonal Matching Pursuit or Basis Pursuit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Advanced topics\n",
    "\n",
    "#### Other method that could utilize sparsity\n",
    "\n",
    "Sparsity naturally comes up in matrix factorization (from using some form of L1 regularization)\n",
    "\n",
    "The general form of matrix factorization\n",
    "\n",
    "$argmin_{A, X} \\rho(Y, AX) + \\lambda \\phi(A) + \\gamma \\psi(X)$ \n",
    "\n",
    "$\\rho$ is most commonly some matrix norm, for example PCA solves problem with $\\rho(Y, AX) = \\|Y - AX\\|^2_F,\\phi, \\psi = 0$\n",
    "\n",
    "* Nonnegative Matrix Factorization (**TODO** link this with NMF on arXiv )\n",
    "\n",
    "* Sparse PCA (like PCA, but $\\psi(X) = \\sum_{i}{\\|X_i\\|_1}$)\n",
    "\n",
    "* Robust PCA ($\\psi(X) = \\sum_{i}{\\|X_i\\|_1}$, $\\phi(A) = \\|A\\|_{*} = tr(\\sqrt{A^T A}) = $ *sum of $A$'s singular values*)\n",
    "\n",
    "* Sparse autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Maximum A Posteriori estimation\n",
    "A precise way to formulate the problem is to pose it as Maximum A Posteriori estimation:\n",
    "\n",
    "Recall that Maximum A Posteriori estimate of parameters $X$ given data $Y$ is defined as maximizing \n",
    "\n",
    "$$\\hat{X} = argmax_{X}P(X | Y)$$\n",
    "\n",
    "Thus expression analogous to negative log-likelihood becomes (with Gaussian noise)\n",
    "\n",
    "$-logP(X | Y) \\propto -log(P(Y|X)P(X)) \\propto -log(e^{\\|Y - DX\\|^2} e^{\\lambda\\|X\\|_1}) =  \\|Y - DX\\|^2 + \\lambda\\|X\\|_1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Resources\n",
    "\n",
    "The go-to person for topics on sparse models in image & signal processing is [Michael Elad](https://elad.cs.technion.ac.il/#), author of:\n",
    "\n",
    "[Five Lectures on Sparse and Redundant Representations](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&ved=2ahUKEwjptojVpsPgAhUNt4sKHfihCIgQFjAAegQIChAC&url=https%3A%2F%2Felad.cs.technion.ac.il%2Fwp-content%2Fuploads%2F2018%2F02%2FPCMI2010-Elad.pdf&usg=AOvVaw3Z1qVcrj-01ikUQU84KkKO) - an introductory article\n",
    "\n",
    "[Sparse and Redundant Representations](https://www.edx.org/professional-certificate/israelx-sparse-representations-from-theory-to-practice) edX Specialization\n",
    "\n",
    "For statistical learning point of view:\n",
    "\n",
    "[Statistical Learning with Sparsity](https://web.stanford.edu/~hastie/StatLearnSparsity/) book - its authors were working with people who came up with these ideas in the first place\n",
    "\n",
    "In context of signal processing these topics are also jointly called *compressed sensing*. People who are working in this branch tend to work on random matrix theory, for example Emanuel Candes and Terence Tao."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
