{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse and redundant representations - notes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Intro\n",
    "\n",
    "The field deals with problems stemming from underdetermined linear systems ($x \\in \\mathbb{R}^n$, $y \\in \\mathbb{R}^m$ where $m < n$)\n",
    "\n",
    "$$Ax = b$$\n",
    "\n",
    "The problem of finding such $x$'s is ill, posed, but there are situations where finding \n",
    "\n",
    "$$x^{*} = \\underset{Ax = b}{argmin}|supp(x)| \\tag{P0}$$\n",
    "\n",
    "Is pretty useful.\n",
    "\n",
    "$supp(x)$ is also denoted $\\|x\\|_0$ (which is abuse of notation, since $\\|\\cdot\\|_0$ is not a norm - it is not homogenous)\n",
    "\n",
    "#### Troubles with solving  $P_0$\n",
    "\n",
    "In general the problem of finding such $x^{*}$ is known to be NP-complete.\n",
    "\n",
    "Because of this approximations are used in practice.\n",
    "\n",
    "There are 2 different approaches to approximately solve $P_0$:\n",
    "\n",
    "- Greedy methods\n",
    "- Relaxation methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Guarantees\n",
    "\n",
    "Though solving $P_0$ exactly is tough, there are conditional guarantees for solutions of corresponding linear system.\n",
    "\n",
    "Define $spark(A)$ as the smallest $s$ such that exist columns $(A_{i_j})_{j < s}$ that are linearly dependent (this is actually $\\|x^{*}\\|_0$ for $P_0$ with $b = 0$). In other words, it's smallest $\\|x\\|_0$ for $x \\in ker(A)$ ($ker(A) = \\{x \\in \\mathbb{R}^n : Ax = 0\\}$).\n",
    "\n",
    "If $x$ is a solution with $\\|x\\|_0 < \\frac{spark(A)}{2}$ then it is the sparsest solution.\n",
    "\n",
    "Proof of the claim above: \n",
    "If $x,y$ are solutions and $max\\{\\|x\\|_0, \\|y\\|_0\\} < \\frac{spark(A)}{2}$,\n",
    "\n",
    "then $0 = Ax - Ay = A(x-y)$, \n",
    "\n",
    "so $\\|x - y\\|_0 \\leq \\|x\\|_0 + \\|y\\|_0  < spark(A)$, a contradiction.\n",
    "\n",
    "#### Upper bound on spark\n",
    "\n",
    "Spark is difficult to compute (computing it in general is NP-hard), but there are methods for upper bounding it by something that is easier to compute.\n",
    "\n",
    "Define *mutual coherence* of column-normalized $A$, \n",
    "\n",
    "$$\\mu(A) = max_{i \\neq j} A^T_i A_j$$\n",
    "\n",
    "**Theorem**\n",
    "\n",
    "$spark(A) \\leq 1 + \\frac{1}{\\mu(A)}$\n",
    "\n",
    "This theorem will use fact that is an easy consequence of \n",
    "\n",
    "**Gershgorin's circle theorem**:\n",
    "\n",
    "Let $A$ be any square matrix. Then its eigenvalues lie in union of disks of the form \n",
    "$ D_k \n",
    "= \\{ x : | x - A_{kk} | \\leq \\sum_{i \\neq k}{|A_{ki}|} \\} $\n",
    "\n",
    "Precisely we'll use the \n",
    "\n",
    "**Fact** \n",
    "\n",
    "If $A$ is diagonally dominant ($\\forall k$ $|A_{kk}| \\geq \\sum_{i \\neq k}{|A_{ki}|}$) then $A$ is positive definite.\n",
    "\n",
    "Proof of the theorem:\n",
    "\n",
    "Without loss of generality assume that $A$ is column-normalized.\n",
    "\n",
    "Let $s = spark(A)$. That means that exists a submatrix $A_S$ where $S$ is some $s$-element subset of column indices, such that $A_S$ is not full rank. \n",
    "\n",
    "That means that $det(A_S) = 0$. It is also equivalent that $det(A_S {A_S}^T) = 0$. Since Gram matrix $A_S {A_S}^T$ is positive semidefinite, it means that it is not positive definite. Using above fact it means that it is not diagonally dominant.\n",
    "\n",
    "Note that diagonal entries of $A_S$ are equal to one. That means that exists $k$ such that\n",
    "\n",
    "$$1 = A_{kk} \\leq \\sum_{i \\neq k}{|A_{ki}|} \\}$$\n",
    "\n",
    "On the other hand, by definition of mutual coherence,  $$\\sum_{i \\neq k}{|A_{ki}|} \\leq (s-1)\\mu({A})$$\n",
    "\n",
    "So $1 \\leq (s-1)\\mu({A})$. By rewriting this we obtain $$s \\geq 1 + \\frac{1}{\\mu(A)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Approximation algorithms\n",
    "\n",
    "**Greedy methods** start with $x_0 = 0$ and  built up $x_{n+1}$ by enlarging support.\n",
    "\n",
    "**Relaxation methods** are based on using continuous optimization tools to relaxations of $\\|\\cdot\\|_0$, for example $\\|\\cdot\\|_1$ is convex and can be also tackled with linear programming tools.\n",
    "\n",
    "\n",
    "### Greedy methods\n",
    "\n",
    "They are called matching pursuit in general.\n",
    "\n",
    "* Least Squares Orthogonal Matching Pursuit (LS-OMP)\n",
    "    - this is essentially stepwise regression\n",
    "    - enlarge support with entry that minimizes LS error of new support\n",
    "* OMP\n",
    "    - enlarge support with entry most correlated with residual\n",
    "    - refit coefficients with LS\n",
    "* Matching Pursuit (MP)\n",
    "    - same as OMP, but without LS step\n",
    "* Weak MP\n",
    "    - needs constant $t$\n",
    "    - find first entry that would minimize error more than $t \\|r\\|_2$\n",
    "* Thresholding\n",
    "    - sort $A^T b$\n",
    "    - add coefficients according to order\n",
    "    \n",
    "The following code implements OMP, MP and WMP in Python (numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pursuit(A, b, n_steps, pursuit_type='omp', wmp_t=None):\n",
    "    def get_next_approximation_mp(**kwargs):\n",
    "        a = kwargs['a_next']\n",
    "        best_column_index = kwargs['best_column_index']\n",
    "        x_approx = kwargs['x_approx']\n",
    "        x_approx[best_column_index] += a * np.linalg.norm(res)\n",
    "        return x_approx\n",
    "\n",
    "    def get_next_approximation_omp(**kwargs):\n",
    "        support = kwargs['support']\n",
    "        x = np.zeros(A.shape[1])\n",
    "        x_star = np.linalg.lstsq(A[:, support], b)[0]\n",
    "        x[support] = x_star\n",
    "        return x\n",
    "    \n",
    "    def get_best_column_wmp(inner_products, wmp_t):\n",
    "        assert wmp_t is not None\n",
    "        for i, p in enumerate(inner_products):\n",
    "            residual_error_norm = np.linalg.norm(res)\n",
    "            if p >= wmp_t * residual_error_norm:\n",
    "                return i\n",
    "        return i\n",
    "    \n",
    "    res = b\n",
    "    support = []\n",
    "    x_approx = np.zeros(A.shape[1])\n",
    "    \n",
    "    get_best_column = np.argmax\n",
    "    if pursuit_type == 'omp':\n",
    "        get_next_approximation = get_next_approximation_omp\n",
    "    elif pursuit_type == 'mp':\n",
    "        get_next_approximation = get_next_approximation_mp\n",
    "    elif pursuit_type == 'wmp':\n",
    "        get_next_approximation = get_next_approximation_mp\n",
    "        get_best_column = lambda products: get_best_column_wmp(products, wmp_t)\n",
    "\n",
    "    for __ in range(n_steps):\n",
    "        inner_products = A.T @ res\n",
    "        best_column_index = get_best_column(inner_products)\n",
    "        a_next = inner_products[best_column_index]\n",
    "        #inner_products[best_column_index] = 0\n",
    "        best_column = A[:, best_column_index]\n",
    "        support.append(best_column_index)\n",
    "        x_approx = get_next_approximation(\n",
    "            a_next=a_next,\n",
    "            best_column_index=best_column_index,\n",
    "            support=support,\n",
    "            x_approx=x_approx)\n",
    "        res = b - A @ x_approx\n",
    "\n",
    "    return support, res, x_approx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Relaxation methods\n",
    "\n",
    "* Gradually smooth $L_0$ norm - approximate it with a sequence of smooth functions for which first (or even second) order methods are feasible\n",
    "* Convex relaxation - replace $L_0$ with $L_1$. This is a convex problem, and since the constraints are linear, it can be solved with linear programming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources\n",
    "\n",
    "[Michael Elad](https://elad.cs.technion.ac.il) is the go-to person for this kind of things. His team prepared the aforementioned edX Course, and also he's a author/coauthor of these resources:\n",
    "\n",
    "* [Five Lectures on Sparse and Redundant Representations Modelling of Images](https://elad.cs.technion.ac.il/wp-content/uploads/2018/02/PCMI2010-Elad.pdf) - highly accessible short booklet\n",
    "* [Sparse and Redundant Representations book](https://www.springer.com/gp/book/9781441970107) - more detailed tha previous report\n",
    "* [From Sparse Solutions of Systems of Equations to Sparse Modeling of Signals and Images](http://www.geintra-uah.org/system/files/review_paper_siam_review.pdf) - more dense and advanced review"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
