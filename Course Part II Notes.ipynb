{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse and redundant representations: from theory to practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparseLand model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many problems in image/signal processing can be written as\n",
    "\n",
    "$y = Cx + \\epsilon$ where $\\epsilon \\sim N(0, \\sigma^2)$\n",
    "\n",
    "where $y$ is observed signal, $x$ is actual signal, and $C$ is some linear operator that 'degrades' signal (blurring, image downsampling et c).\n",
    "\n",
    "If we have a prior $P(x)$ we can approach this problem as \n",
    "\n",
    "$\\hat{x} = argmin_{x} P(x) \\text{ such that } \\|Cx - y\\|^2 < \\sigma^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Types of priors\n",
    "\n",
    "Note: for images at least, I think that prior to wavelets, these methods actually were used as priors on image patches rather than whole images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Smoothness-inducing \n",
    "\n",
    "    $P(x) \\propto e^{-\\|Lx\\|^2}$ ($L$ is Laplace operator, discrete 2nd order derivative)\n",
    "\n",
    "* Smoothness-inducing with edge information\n",
    "\n",
    "    $P(x) \\propto e^{-\\|LWx\\|^2}$ ($W$ is diagonal matrix that has smaller values for edge locations)\n",
    "\n",
    "* Total variation\n",
    "\n",
    "    $P(x) \\propto e^{-\\|\\nabla x\\|_1}$\n",
    "\n",
    "* Transform based\n",
    "\n",
    "    $P(x) \\propto e^{-\\|T x\\|^2}$, $T$ is transform coefficient matrix, for example for Fourier transform\n",
    "\n",
    "* Wavelet-based, inducing sparsity\n",
    "\n",
    "    $P(x) \\propto e^{-\\|Wx\\|_1}$ where $W$ is wavelet coefficient matrix, for example for Haar wavelets\n",
    "\n",
    "* Sparseland - dictionary learning (?)\n",
    "\n",
    "    $P(x) \\propto e^{-\\|D\\alpha\\|_1}$ where $D$ is dictionary coefficient matrix\n",
    "\n",
    "In this terminology $W$ matrix generalizes $T$ matrix, and $D$ matrix generalizes further, because the dictionary can either fixed or learned from previous data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SparseLand and other generative methods\n",
    "\n",
    "SparseLand is related to union of subspaces method (subspaces are defined by $D$'s columns), Gaussian Mixture Models, and also local PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use case: Image deblurring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $v$ denote image, and $H$ the blurring operator.\n",
    "\n",
    "The observed image is given by $z = Hv + \\epsilon$ w\n",
    "\n",
    "Since $v$ is an image, we can choose a dictionary $D$ such that $D\\alpha \\approx v$\n",
    "\n",
    "Thus we can formulate the problem (this is dual formulation)\n",
    "\n",
    "$\\alpha = argmin_{\\alpha} \\lambda \\| \\alpha \\|_{0} + \\|HD\\alpha - z\\|^2$\n",
    "\n",
    "In practice $\\| \\dot \\|_0$ is replaced by $\\|\\dot\\|_1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Used optimization methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* gradient descent (with momentum)\n",
    "* Minimization-Majorization:\n",
    "    Idea: for function $f(x)$ hard to minimize, optimize $Q(x, x_0)$ instead\n",
    "    $Q$ needs to satisfy\n",
    "    * $Q(\\alpha_0, \\alpha_0) = f(\\alpha_0)$\n",
    "    * $\\forall{\\alpha} Q(\\alpha, \\alpha_0) = f(\\alpha)$\n",
    "    * $\\nabla Q(\\alpha, \\alpha_0) = \\nabla f(\\alpha)$\n",
    "    \n",
    "    The minimization proceeds by updating  $\\alpha_{i+1} = \\alpha$ where $\\alpha$ is optimal in previous step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionary Learning\n",
    "\n",
    "Dictionary learning is what is done when the transform used in prior is to be learned from data.\n",
    "\n",
    "Technically dictionary learning can be thought of as matrix factorization:\n",
    "\n",
    "$\\hat{A} = argmin_{A}\\|Y - XA\\|^2$ such that $\\forall{i} \\|X_i\\|_p \\leq k$ ($p$ is either 0 or 1) \n",
    "\n",
    "Most methods alternate $X$ and $A$ updates, and use either reconstruction error or sparsity of $A$ as stopping criterion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The denoising problem\n",
    "\n",
    "Denoising problem, formulated as \n",
    "\n",
    "$z = x + v$ where entries of $v$ are $\\mathcal{N}(0, \\sigma^2)$ distributed\n",
    "\n",
    "Is the simplest and important inverse problem.\n",
    "\n",
    "Denoising images is important, because real-world images taken with camera suffer from noise because of lighting condition, compression issues and other.\n",
    "\n",
    "Good denoising methods can be also used not only for removing noise - for example some of them can be used for cartooning, image decomposition, expanding dynamic range, or dehazing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple denoising methods\n",
    "\n",
    "Thresholding"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
